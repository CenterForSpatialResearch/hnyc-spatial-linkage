{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disambiguation 1850.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-biNqiYJ3_hk",
        "O-nklL9S4Qqt",
        "U0d5OKgm5DYs",
        "ENVsP89o4sf_",
        "Y5juB3JY54EZ",
        "A__HwRP4lYRD",
        "zH5NytekY_kt",
        "BFgbkHejiPP-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX-wpYYBQfZf"
      },
      "source": [
        "!pip install pyjarowinkler\n",
        "!pip install haversine\n",
        "!pip install hdbscan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-biNqiYJ3_hk"
      },
      "source": [
        "### write processing module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWsl-h_2QWZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2914b168-2981-4c4e-c9e9-1d906a3fe6d9"
      },
      "source": [
        "# processing\n",
        "%%writefile processing.py\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from pyjarowinkler import distance\n",
        "from haversine import haversine, Unit\n",
        "import time\n",
        "\n",
        "\n",
        "def col_for_disamb(df, cd_id, cen_id, cd_fn=\"CD_FIRST_NAME\", cen_fn=\"CENSUS_FIRST_NAME\", cd_ln=\"CD_LAST_NAME\",\n",
        "                           cen_ln=\"CENSUS_LAST_NAME\", cen_occ=\"CENSUS_OCCUPATION\", cen_age=\"CENSUS_AGE\"):\n",
        "    # name jw dist\n",
        "    df[\"jw_fn\"] = df.apply(lambda x: distance.get_jaro_distance(x[cd_fn], x[cen_fn], winkler=True, scaling=0.1), axis=1)\n",
        "    df[\"jw_ln\"] = df.apply(lambda x: distance.get_jaro_distance(x[cd_ln], x[cen_ln], winkler=True, scaling=0.1), axis=1)\n",
        "    df[\"jw_score\"] = 0.4 * df[\"jw_fn\"] + 0.6 * df[\"jw_ln\"]\n",
        "\n",
        "    # occ\n",
        "    df['occ_listed'] = np.where((df[cen_occ].isnull()) | (df[cen_occ] == '*'), 0, 1)\n",
        "\n",
        "    # age\n",
        "    df['age_score'] = np.where(df[cen_age] <= 12, 0, 1)\n",
        "\n",
        "    # cd conflicts\n",
        "    df[\"cd_count\"] = df.groupby(cd_id)[cen_id].transform('count')\n",
        "    df[\"census_count\"] = df.groupby(cen_id)[cd_id].transform('count')\n",
        "\n",
        "    df['census_count_inverse'] = 1 / df['census_count']\n",
        "    df['cd_count_inverse'] = 1 / df['cd_count']\n",
        "\n",
        "    #This is so the bipartite matching algorthm works the way we need it to\n",
        "    df['CD_ID'] = 'CD_' + df[cd_id].astype(str)\n",
        "    df['CENSUS_ID'] = 'CENSUS_' + df[cen_id].astype(str)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Applies confidence score to df\n",
        "\"\"\"\n",
        "def apply_confidence_score(df, cd_fn = \"CD_FIRST_NAME\", cen_fn = \"CENSUS_FIRST_NAME\", cd_ln = \"CD_LAST_NAME\", cen_ln = \"CENSUS_LAST_NAME\", cen_occ = \"CENSUS_OCCLABELB\", cen_age = \"CENSUS_AGE\", cd_id=\"OBJECTID\", cen_id=\"OBJECTID.x\"):\n",
        "    \n",
        "    # name jw dist\n",
        "    df[\"jw_fn\"] = df.apply(lambda x: distance.get_jaro_distance(x[cd_fn], x[cen_fn], winkler=True, scaling=0.1), axis = 1)\n",
        "    df[\"jw_ln\"] = df.apply(lambda x: distance.get_jaro_distance(x[cd_ln], x[cen_ln], winkler=True, scaling=0.1), axis = 1)\n",
        "    df[\"jw_score\"] = 0.4 * df[\"jw_fn\"] + 0.6 * df[\"jw_ln\"]\n",
        "\n",
        "    # occ\n",
        "    df['occ_listed'] = np.where((df[cen_occ].isnull()) | (df[cen_occ] == '*'), 0, 1)\n",
        "\n",
        "    # age\n",
        "    df['age_score'] = np.where(df[cen_age] <= 12, 0, 1)\n",
        "\n",
        "    # cd conflicts\n",
        "    df[\"cd_count\"] = df.groupby(cd_id)[cen_id].transform('count')\n",
        "    df[\"census_count\"] = df.groupby(cen_id)[cd_id].transform('count')\n",
        "\n",
        "    df['confidence_score'] = .5*df.jw_score + .2*(1/df.cd_count) + \\\n",
        "                             .2*(1/df.census_count) + .05*df.occ_listed + \\\n",
        "                             .05*df.age_score\n",
        "    df['confidence_score'] = df['confidence_score'].round(decimals = 2)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "#not needed in new run\n",
        "\"\"\"\n",
        "Takes elastic search and census directory geocode file to create a dataframe \n",
        "ready for the disambiguation process.\n",
        "Can add/incorporate new columns to include in confidence score here\n",
        "elastic_search: either df with elastic search output or elastic search output file\n",
        "city_directory: either df with city directory data or file name\n",
        "file: boolean value, set to True if elastic_search/city_directory are file names otherwise set \n",
        "to false. Default false \n",
        "\"\"\"\n",
        "def elastic_to_disamb(elastic_search, city_directory, file = False):\n",
        "\n",
        "    if file:\n",
        "        elastic_search = pd.read_csv(elastic_search, sep='\\t', engine='python')\n",
        "        city_directory = pd.read_csv(city_directory)\n",
        "\n",
        "    else:\n",
        "        elastic_search = elastic_search.copy()\n",
        "        city_directory = city_directory.copy()\n",
        "\n",
        "    latlng = city_directory[['OBJECTID', 'LONG', 'LAT']]\n",
        "\n",
        "    #print(elastic_search.head())\n",
        "    #print(latlng.head())\n",
        "\n",
        "    match = apply_confidence_score(elastic_search, cen_fn='CENSUS_NAMEFRSTB', cen_ln='CENSUS_NAMELASTB',\n",
        "                                      cen_occ='CENSUS_OCCLABELB', cen_id='OBJECTID.x')\n",
        "\n",
        "    #print(match.head())\n",
        "    match['CD_ID'] = 'CD_' + match['OBJECTID'].astype(str)\n",
        "    match['CENSUS_ID'] = 'CENSUS_' + match['OBJECTID.x'].astype(str)\n",
        "\n",
        "    #Can remove this after finalizing the confidence score\n",
        "    match['census_count_inverse'] = 1 / match['census_count']\n",
        "    match['cd_count_inverse'] = 1 / match['cd_count']\n",
        "\n",
        "    match = match.merge(latlng, how='left', on='OBJECTID', validate='many_to_one')\n",
        "    #print(match.head())\n",
        "    return match\n",
        "\"\"\"\n",
        "Create a list of dataframes where the top row is an anchor\n",
        "Each dataframe is one where spatial disambiguation will be applied\n",
        "This is necessary as else, algorithms take too long to run\n",
        "Match: df of matches\n",
        "confidence_score: name of confidence score column\n",
        "\"\"\"\n",
        "\n",
        "def split_dfs(match, sort_var=\"CENSUS_ID\", confidence=\"confidence_score\"):\n",
        "\n",
        "    match = match.sort_values(by=[sort_var])\n",
        "\n",
        "    # identify anchors and assign anchor ID\n",
        "    match['anchor'] = np.where(match[confidence] == 1, 1, None)\n",
        "    sub_group = pd.DataFrame({'index': list(match.loc[match.anchor.notnull(), :].index), 'group_ID': range(0, sum(match['anchor'].notnull()))}).set_index('index')\n",
        "    match = match.join(sub_group)\n",
        "    match['group_ID'] = match['group_ID'].fillna(method='ffill').fillna(method='backfill')\n",
        "\n",
        "    # split df into multiple df, each bounded by anchor\n",
        "\n",
        "    # sub_group_dict = {group: df for group, df in match.groupby('group_ID')}\n",
        "    sub_groups = [df for group, df in match.groupby('group_ID')]\n",
        "    \n",
        "    # add bottom anchor back\n",
        "    \"\"\"\n",
        "    for i in range(0, len(sub_group_dict) - 1):\n",
        "        sub_group_dict[i] = pd.concat([sub_group_dict[i], sub_group_dict[i+1][0:1]])\n",
        "    \"\"\"\n",
        "    return sub_groups\n",
        "\n",
        "\"\"\"\n",
        "Create node ID for each match, to be using the shortest path algorithm \n",
        "sub must be a df with each row as a potential match between a CD and census record. It must contain the columns CD_ID, CENSUS_ID, LONG, LAT, confidence_score and MATCH_ADDR.\n",
        "the column names can be specified individually if they are named differently\n",
        "Returns the dataframe with new columns, 'anchor', 'node_ID' and 'letter'.\n",
        "anchor: whether row is an anchor (confidence score = 1)\n",
        "node_ID: unique node ID. each node is a match, so e.g. A0 and A1 refers to two potential CD matches for the same census record\n",
        "letter: grouping for identical census records \n",
        "add_prefixes: whether to add prefixes 'CD_' and 'CENSUS_' to cd_id and census_id respectively. prefixes are required for subsequent bipartite matching\n",
        "\"\"\"\n",
        "def create_path_df(sub_graph, census_id = \"CENSUS_ID\"):\n",
        "\n",
        "    sub_graph['node_ID'] = sub_graph.groupby(census_id).cumcount()\n",
        "\n",
        "    letter_id = sub_graph[census_id].unique().tolist()\n",
        "    letters = ['N' + str(x) for x in range(0, len(letter_id))]\n",
        "    letter_id = pd.DataFrame({'CENSUS_ID': letter_id, 'letter': letters})\n",
        "\n",
        "    sub_graph = sub_graph.merge(letter_id, how='left', left_on=census_id, right_on=\"CENSUS_ID\", validate='many_to_one')\n",
        "\n",
        "    sub_graph['node_ID'] = sub_graph.apply(lambda row: row.letter + '_' + str(row.node_ID), axis=1)\n",
        "\n",
        "    return sub_graph\n",
        "\n",
        "\"\"\"\n",
        "Creates a graph from the sub_graph dataframe\n",
        "Each node being a potential CD-census match and \n",
        "    each edge being the link between the potential CD records of consecutive census records\n",
        "The weight of each edge = the haversine distance between the two\n",
        "cluster_col: name of column with cluster group. If does not exist, use None\n",
        "Returns the graph object\n",
        "\"\"\"\n",
        "\n",
        "def create_path_graph(g, cluster_col='in_cluster_x', lat='CD_X', lon='CD_Y'):\n",
        "\n",
        "    g.loc[:, 'key'] = 0\n",
        "    g = g.merge(g, on='key')\n",
        "\n",
        "    #This is time consuming\n",
        "    g['key'] = g.apply(lambda row: 1 if int(row.letter_x[1:]) - int(row.letter_y[1:]) == -1 else 0, axis = 1)\n",
        "\n",
        "    g = g[g.key == 1]\n",
        "\n",
        "    g['weight'] = g.apply(lambda row: haversine((row[lat + '_y'], row[lon + '_y']), (row[lat + '_x'], row[lon + '_x']), unit=Unit.METERS), axis=1)\n",
        "\n",
        "    if cluster_col != None:\n",
        "        g['weight'] = g.apply(lambda row: row.weight + 999 if row[cluster_col] == -1 else row.weight, axis=1)\n",
        "\n",
        "    g_edges = [(row.node_ID_x, row.node_ID_y, row.weight) for index, row in g.iterrows()]\n",
        "    graph = nx.DiGraph()\n",
        "    graph.add_weighted_edges_from(g_edges)\n",
        "\n",
        "    \n",
        "    return graph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing processing.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-nklL9S4Qqt"
      },
      "source": [
        "### write disambiguation module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEk0C_rDR8PN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1567230-a1d2-48b7-fa0a-b15d73175a9c"
      },
      "source": [
        "# disambiguation\n",
        "%%writefile disambiguation.py\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import hdbscan\n",
        "from itertools import islice\n",
        "#import disambiguation.processing as dp\n",
        "import processing as dp\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Wrapper function for everything below, including checks\n",
        "Designed to work within list comprehension only! (refer to Disambiguator())\n",
        "Works by applying algorithm to specified df (using index i) in the list\n",
        "sub_groups: list of dfs, each df being a subset of the census bounded by 2 anchors\n",
        "i: index of df in the list\n",
        "\"\"\"\n",
        "def apply_algo(sub_groups, i, cluster=True, k_between=True, census_id='CENSUS_ID', census_count=\"census_count\", confidence='confidence_score', lat=\"CD_X\", lon=\"CD_Y\", cluster_kwargs={}, path_kwargs={}):\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(\"Reached: \" + str(i))\n",
        "    df = sub_groups[i]\n",
        "    if sum(df[census_count] > 1) == 0: # no disambiguation needed\n",
        "        return df\n",
        "\n",
        "    if i + 1 < len(sub_groups): # add bottom anchor\n",
        "        df = pd.concat([df, sub_groups[i+1][0:1]]) \n",
        "\n",
        "    path_df = dp.create_path_df(df, census_id)\n",
        "\n",
        "\n",
        "    if cluster:\n",
        "        # apply density clustering and remove outlier nodes\n",
        "        path_df = apply_density_clustering(path_df, lat, lon, **cluster_kwargs)\n",
        "        cluster_arg = 'in_cluster_x'\n",
        "    else:\n",
        "        cluster_arg = None\n",
        "\n",
        "    # create graph and k shortest paths centrality\n",
        "\n",
        "    g = dp.create_path_graph(path_df, cluster_col=cluster_arg, lat=lat, lon=lon)\n",
        "\n",
        "    if k_between:\n",
        "\n",
        "        output = apply_k_betweenness(path_df, g, confidence = confidence, **path_kwargs)\n",
        "    else:\n",
        "        output = apply_shortest_path(path_df, g, confidence = confidence, **path_kwargs)\n",
        "\n",
        "    return output\n",
        "\n",
        "\"\"\"\n",
        "Apply Dijkstra's algorithm to the graph and get spatial weights\n",
        "Spatial weights are computed as confidence score +1 if match was included in shortest path, and confidence score + 0 otherwise\n",
        "df: dataframe of records with confidence score and node ID, names can be modified via parameters\n",
        "graph: graph object created from create_path_graph()\n",
        "source: start node, e.g. 'A0'. By default it chooses first row in the table\n",
        "target: end node, e.g. 'J0'. By default it chooses last row in the table\n",
        "Returns a dataframe of matches with added 'spatial weight'\n",
        "\"\"\"\n",
        "def apply_shortest_path(df, graph, source = None, target = None, confidence = 'confidence_score', node_id = 'node_ID'):\n",
        "    if source == None:\n",
        "        source = list(df[node_id])[0]\n",
        "    if target == None:\n",
        "        target = list(df[node_id])[-1]\n",
        "\n",
        "    path = nx.dijkstra_path(graph, source, target)\n",
        "    df['spatial_weight'] = df.apply(lambda row: row[confidence] + 1 if row[node_id] in path else row[confidence], axis = 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\"\"\"\n",
        "Apply betweenness centrality using k shortest paths (as documented in spatial_disambiguation.ipynb)\n",
        "df: df of matches\n",
        "graph: graph object created from create_path_graph()\n",
        "source: start node, e.g. 'A0'. By default it chooses first row in the table\n",
        "target: end node, e.g. 'J0'. By default it chooses last row in the table\n",
        "k: how many shortest paths to choose from (absolute number). By default 1 or ~ 1/2 of number of possible paths if there are more than 30 paths\n",
        "scale: how much to scale the score by when adding it with confidence score. Default = 1 (equal weight of confidence score and spatial weight)\n",
        "Returns\n",
        "    df: df with spatial weights column\n",
        "    k_paths: paths used for calculation\n",
        "\"\"\"\n",
        "def apply_k_betweenness(df, graph, confidence = \"confidence_score\", source=None, target=None, k=None, scale=1):\n",
        "    if source == None:\n",
        "        source = list(df[\"node_ID\"])[0]\n",
        "    if target == None:\n",
        "        target = list(df[\"node_ID\"])[-1]\n",
        "\n",
        "    k_paths = nx.shortest_simple_paths(graph, source, target, weight=\"weight\")\n",
        "\n",
        "    length = get_n_paths(df)\n",
        "    if k == None:\n",
        "        if length < 31:\n",
        "            k = 1\n",
        "        elif length > 50:\n",
        "            k = 50\n",
        "        else:\n",
        "            k = int(0.5 * length)\n",
        "\n",
        "    k_paths = list(islice(k_paths, k))\n",
        "\n",
        "    # initialize output: dict with nodes as keys\n",
        "    spatial_weights = dict.fromkeys(graph.nodes, 0)\n",
        "    \n",
        "    # count\n",
        "    for path in k_paths:\n",
        "        for node in path:\n",
        "            spatial_weights[node] += 1\n",
        "    \n",
        "    spatial_weights = [[key , round(value / k, 2) * scale] for key, value in spatial_weights.items()]\n",
        "    spatial_df = pd.DataFrame(spatial_weights, columns=[\"node_ID\", 'spatial_weight'])\n",
        "    df = df.merge(spatial_df, how=\"inner\", on=\"node_ID\", validate=\"one_to_one\")\n",
        "    df['spatial_weight'] = df['spatial_weight'] + df[confidence]\n",
        "\n",
        "    return df\n",
        "\n",
        "\"\"\"\n",
        "Helper method to count the number of possible paths in the graph\n",
        "\"\"\"\n",
        "def get_n_paths(df):\n",
        "    k = 1\n",
        "    counts = df.groupby('letter')['letter'].size().to_list()\n",
        "    for count in counts:\n",
        "        k *= count\n",
        "    \n",
        "    return k\n",
        "\n",
        "\"\"\"\n",
        "Apply density based clustering to detect outliers. Requires `hdbscan` library\n",
        "Refer to hdbscan documentation on parameters\n",
        "Returns df with a column 'in_cluster' indicating which cluster the nodes are in\n",
        "\"\"\"\n",
        "def apply_density_clustering(df, lat='CD_X', lon=\"CD_Y\", min_cluster_size=10, min_samples=10, allow_single_cluster=True, **kwargs):\n",
        "    cluster_sub = df.loc[:, [lon, lat]]\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, allow_single_cluster=allow_single_cluster, **kwargs).fit(cluster_sub)\n",
        "\n",
        "    df['in_cluster'] = pd.Series(clusterer.labels_).values\n",
        "    return df\n",
        "\n",
        "\"\"\"\n",
        "A bipartite graph is created from the matches, with each node being either a census or CD record and each edge indicating a potential match. \n",
        "Note that subgraph MUST have prefixes on the cd_id ('CD_') and census_id ('CENSUS_') columns\n",
        "The matching algorithm (maximum weighted matching) will \n",
        "    (1) select sets of matches that give the highest number of matches \n",
        "    (2) choose the match set that has the highest weight based on that\n",
        "Returns a dictionary with 'graph' as the list of bipartite graphs and 'results' being the original df with an additional 'selected' column, indicating the correct match and 'graph_id' column, indicated subgraph.\n",
        "\"\"\"\n",
        "def get_matches(df, cd_id = 'CD_ID', census_id = 'CENSUS_ID', weight = 'spatial_weight'):\n",
        "    b_edges = [(row[cd_id], row[census_id], row[weight]) for index, row in df.iterrows()]\n",
        "    b = nx.Graph()\n",
        "    b.add_weighted_edges_from(b_edges)\n",
        "\n",
        "    # algorithm is too expensive if we perform it on entire graph. moreover, graph is actually disconnected into sub_graphs. apply algorithm on subgraphs instead\n",
        "    subgraphs = [b.subgraph(c) for c in nx.connected_components(b)]\n",
        "    matches = [list(nx.max_weight_matching(graph, maxcardinality = True)) for graph in subgraphs]\n",
        "    matches = [sorted(list(item)) for sublist in matches for item in sublist] # unnest and convert pairs from tuple to list\n",
        "    matches = pd.DataFrame(matches, columns=[cd_id, census_id])\n",
        "    matches['selected'] = 1\n",
        "\n",
        "    df = df.merge(matches, how='left', on=[cd_id, census_id], validate='one_to_one')\n",
        "    df['selected'] = df['selected'].fillna(0)\n",
        "\n",
        "    # add subgraph id\n",
        "    subgraph_id = [{'graph_ID': i, 'CD_ID': node} for i in range(0, len(subgraphs)) for node in list(subgraphs[i].nodes) if node[:2] == 'CD']\n",
        "    subgraph_id = pd.DataFrame(subgraph_id)\n",
        "    df = df.merge(subgraph_id, how=\"inner\", left_on=cd_id, right_on=\"CD_ID\", validate=\"many_to_one\")\n",
        "\n",
        "    return {'graph': subgraphs, 'results': df}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing disambiguation.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0d5OKgm5DYs"
      },
      "source": [
        "### write analysis module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii5TyopzR8QF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c29d141-e550-4345-9bc8-79c7ce051758"
      },
      "source": [
        "# analysis\n",
        "%%writefile analysis.py\n",
        "\n",
        "from haversine import haversine, Unit\n",
        "from numpy import log\n",
        "#import disambiguation.disambiguation as dl\n",
        "import disambiguation as dl\n",
        "\n",
        "\"\"\"\n",
        "Get number of selected matches, out of total possible (ie unique CD records)\n",
        "df: df with \"selected\" column, after running get_matches()\n",
        "cd_id: name of cd_id column\n",
        "\"\"\"\n",
        "def get_match_rate(df, cd_id='CD_ID'):\n",
        "    n_cd_records = len(df[cd_id].unique())\n",
        "    n_selected = sum(df[\"selected\"].values)\n",
        "    match_rate = round(n_selected / n_cd_records * 100, 2)\n",
        "\n",
        "    return match_rate\n",
        "\n",
        "\"\"\"\n",
        "Get number of perfect matches (in terms of address) selected\n",
        "df: df with \"selected\" column, after running get_matches()\n",
        "cd_add: name of cd address column\n",
        "cen_add: name of cen_add column\n",
        "\"\"\"\n",
        "def get_addr_success(df, cd_add='MATCH_ADDR', cen_add='CENSUS_MATCH_ADDR'):\n",
        "    df['cd_add_cln'] = df.apply(lambda row: row[cd_add][:row[cd_add].index(',')], axis=1)\n",
        "    df['cen_add_cln'] = df.apply(lambda row: row[cen_add][:row[cen_add].index(',')], axis=1)\n",
        "    n_perfect_match_chosen = len(df.loc[(df['cd_add_cln'] == df['cen_add_cln']) & (df['selected'] == 1), :])\n",
        "    n_perfect_match = len(df.loc[df['cd_add_cln'] == df['cen_add_cln'], :])\n",
        "\n",
        "    return {'n_perfect_match_chosen': n_perfect_match_chosen, 'n_perfect_match': n_perfect_match}\n",
        "\n",
        "\"\"\"\n",
        "Get error rate based on distance (in metres) between matched and actual address\n",
        "df: df with \"selected\" column, after running get_matches()\n",
        "cen_lon: name of census long column\n",
        "cen_lat: name of census lat column\n",
        "lon: name of long column\n",
        "lat: name of lat column\n",
        "\"\"\"\n",
        "def get_dist_error(df, cen_lon='CENSUS_X', cen_lat='CENSUS_Y', lon='CD_X', lat='CD_Y'):\n",
        "    df['dist'] = df.apply(lambda row: haversine((row[cen_lat], row[cen_lon]), (row[lat], row[lon]), unit=Unit.METERS), axis=1)\n",
        "    return df\n",
        "\n",
        "\"\"\"\n",
        "Get number of selected matches, out of total possible (ie unique CD records)\n",
        "df: df with \"selected\" column, after running get_matches()\n",
        "cd_id: name of cd_id column\n",
        "\"\"\"\n",
        "def get_under12_selections(df, age='CENSUS_AGE'):\n",
        "    n_under12 = len(df.loc[(df[age] <= 12) & (df['selected'] == 1), :])\n",
        "    n_selected = len(df.loc[df['selected'] == 1, :])\n",
        "    proportion = round(n_under12 / n_selected * 100, 2)\n",
        "\n",
        "    return proportion\n",
        "\n",
        "\"\"\"\n",
        "Get df containing selected matches based on actual distances and confidence score\n",
        "df: any match df containing at least cd_id, census_id, census long/lat, cd long/lat and confidence score. preferably df with 'dist' column (after get_dist_error())\n",
        "\"\"\"\n",
        "def get_dist_based_match(df, cen_lon='CENSUS_X', cen_lat='CENSUS_Y', lon='CD_X', lat='CD_Y', cd_id='CD_ID', census_id='CENSUS_ID', confidence='confidence_score'):\n",
        "    if 'dist' not in df.columns:\n",
        "        df = get_dist_error(df, cen_lon=cen_lon, cen_lat=cen_lat, lon=lon, lat=lat)\n",
        "    \n",
        "    df['dist_weight'] = round(1 / log(df['dist']) + df[confidence], 2)\n",
        "    dist_disamb = dl.get_matches(df, cd_id = cd_id, census_id = census_id, weight = 'dist_weight')\n",
        "\n",
        "    return dist_disamb\n",
        "\n",
        "\"\"\"\n",
        "Get false positive and false negative rates\n",
        "df_algo: df with \"selected\" column, after running get_matches()\n",
        "df_dist: df with \"selected\" column, after running get_dist_based_match()\n",
        "Returns confusion matrix and df_algo with 'selected' now called 'selected_algo', and an additional column, 'selected_dist' which indicates 'true' matches.\n",
        "\"\"\"\n",
        "def compare_selections(df_algo, df_dist, cd_id=\"CD_ID\", census_id=\"CENSUS_ID\"):\n",
        "    df_algo = df_algo.merge(df_dist.loc[:, [cd_id, census_id, 'selected']], how=\"inner\", on=[cd_id, census_id], validate='one_to_one', suffixes=('_algo', '_dist'))\n",
        "\n",
        "    true_positive = len(df_algo.loc[(df_algo['selected_algo'] == 1) & (df_algo['selected_dist'] == 1), :])\n",
        "    false_positive = len(df_algo.loc[(df_algo['selected_algo'] == 1) & (df_algo['selected_dist'] == 0), :])\n",
        "    false_negative = len(df_algo.loc[(df_algo['selected_algo'] == 0) & (df_algo['selected_dist'] == 1), :])\n",
        "    true_negative = len(df_algo.loc[(df_algo['selected_algo'] == 0) & (df_algo['selected_dist'] == 0), :])\n",
        "\n",
        "    confusion_matrix = [[true_positive, false_positive], [false_negative, true_negative]]\n",
        "    return {'confusion_matrix': confusion_matrix, 'merged_df': df_algo}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing analysis.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENVsP89o4sf_"
      },
      "source": [
        "### write benchmarking module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9jue_siR8Qx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a652c16-7e53-4d12-e59d-09235d862e7e"
      },
      "source": [
        "# benchmark\n",
        "%%writefile benchmark.py\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "#import disambiguation.analysis as da\n",
        "import analysis as da\n",
        "\n",
        "\n",
        "#Static methods\n",
        "\n",
        "def get_hn(add):\n",
        "    hn = re.search('[0-9]+', add)\n",
        "    return hn.group()\n",
        "\n",
        "def get_st(add):\n",
        "    try:\n",
        "        st = re.search('(?<=[0-9]\\\\s)([A-Z]|\\\\s)+(?=,)', add)\n",
        "        return st.group()\n",
        "    except:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing benchmark.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5juB3JY54EZ"
      },
      "source": [
        "### write init module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIKHd5BG566d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f33414f-8e8f-4534-da11-8a6282b046ab"
      },
      "source": [
        "%%writefile __init__.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import processing as dp\n",
        "import disambiguation as dl\n",
        "import analysis as da\n",
        "import benchmark as bm\n",
        "import time\n",
        "\n",
        "class Disambiguator:\n",
        "\n",
        "    def __init__(self, match_df, cd_id=\"CD_ID\", census_id=\"CENSUS_ID\", confidence=\"confidence_score\", census_count='census_count', lon=\"CD_Y\", lat=\"CD_X\", sort_var=\"CENSUS_ID\"):\n",
        "        # initialize input \n",
        "        self.input = match_df\n",
        "\n",
        "        # initialize col names\n",
        "        self.sort_var = sort_var\n",
        "        self.cd_id = cd_id\n",
        "        self.census_id = census_id\n",
        "        self.cen_count = census_count\n",
        "        self.confidence = confidence\n",
        "        self.lon = lon\n",
        "        self.lat = lat\n",
        "\n",
        "        # output variables \n",
        "        self.bipartite = None\n",
        "        self.results = None\n",
        "\n",
        "    def run_disambiguation(self, cluster=True, k_between=True, cluster_kwargs = {}, path_kwargs = {}):\n",
        "        start_time = time.time()\n",
        "        print(\"Running\")\n",
        "\n",
        "        print(\"Creating dictionary of sub dfs (1/4)...\")\n",
        "        sub_groups = dp.split_dfs(self.input, self.sort_var, self.confidence)\n",
        "\n",
        "        print(\"Applying algorithms iteratively (2/4)...\")\n",
        "        print(\"Number of Subgraphs: \" + str(len(sub_groups)))\n",
        "\n",
        "\n",
        "        # iteratively apply algorithms onto each sub df\n",
        "        sub_groups = [dl.apply_algo(sub_groups, i, cluster=cluster, census_id=self.census_id, confidence=self.confidence, lat=self.lat, lon=self.lon, k_between=k_between, cluster_kwargs=cluster_kwargs, path_kwargs=path_kwargs) for i in range(0, len(sub_groups))]\n",
        "\n",
        "        print(\"Cleaning output (3/4)...\")\n",
        "        final = pd.concat(sub_groups)\n",
        "\n",
        "        final = final.drop_duplicates([self.cd_id, self.census_id])\n",
        "\n",
        "        final['anchor'] = final['anchor'].fillna(0)\n",
        "        final['spatial_weight'] = np.where(final['spatial_weight'].isnull(), final[self.confidence] + 1, final['spatial_weight']) # conf + 1 when weight is null - these are the rows that had did not req. spatial disambiguation, hence would def. be in shortest path\n",
        "\n",
        "        print(\"Disambiguating (4/4)...\")\n",
        "        disambiguated = dl.get_matches(final)\n",
        "        self.bipartite = disambiguated['graph']\n",
        "        self.results = disambiguated['results']\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(\"Total time:\", end_time - start_time)\n",
        "        print(\"Done! :)\")\n",
        "    \n",
        "    def get_result(self):\n",
        "        return self.results\n",
        "\n",
        "    def get_bgraph(self):\n",
        "        return self.bipartite\n",
        "\n",
        "    def save_result(self, output_fn):\n",
        "        self.results.to_csv(output_fn, index=False)\n",
        "\n",
        "class Disambiguator1880(Disambiguator):\n",
        "    def __init__(self, match_df, cd_id='CD_ID', census_id='CENSUS_ID', confidence='confidence_score', lon='LONG', lat='LAT'):\n",
        "        super().__init__(match_df, cd_id=cd_id, census_id=census_id, confidence=confidence, lon=lon, lat=lat)\n",
        "        self.cen_lon = None\n",
        "        self.cen_lat = None\n",
        "        self.cen_add = None\n",
        "        self.cd_add = None\n",
        "\n",
        "    # adding in variables needed for analysis\n",
        "    def merge_census_var(self, df, merge_cen_id=\"CENSUS_ID\"):\n",
        "        try:\n",
        "           self.results = self.results.merge(df, how=\"left\", left_on=self.census_id, right_on=merge_cen_id)\n",
        "\n",
        "        except AttributeError:\n",
        "            raise Exception(\"Please run run_disambiguation() first\")\n",
        "\n",
        "    def merge_cd_var(self, df, merge_cd_id=\"CD_ID\"):\n",
        "        try:\n",
        "            self.results = self.results.merge(df, how=\"left\", left_on=self.cd_id, right_on=merge_cd_id)\n",
        "\n",
        "        except AttributeError:\n",
        "            raise Exception(\"Please run run_disambiguation() first\")\n",
        "\n",
        "    def set_var(self, var= None):\n",
        "        if var is None:\n",
        "            var = {'cen_lon': 'CENSUS_X', 'cen_lat': 'CENSUS_Y', 'cen_add': 'CENSUS_MATCH_ADDR', 'cd_add': 'MATCH_ADDR'}\n",
        "\n",
        "        for key, value in var.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "    # functions for analysis\n",
        "    def get_match_rate(self):\n",
        "        \n",
        "        return da.get_match_rate(self.results, cd_id=self.cd_id)\n",
        "\n",
        "    def get_addr_success(self):\n",
        "        if self.cd_add is None or self.cen_add is None:\n",
        "            raise Exception(\"Please check that cd_add and cen_add have been initialized through set_var\")\n",
        "\n",
        "        return da.get_addr_success(self.results, cd_add=self.cd_add, cen_add=self.cen_add)\n",
        "\n",
        "    def get_dist_error(self):\n",
        "        if self.cen_lon is None or self.cen_lat is None:\n",
        "            raise Exception(\"Please check that cen_lon and cen_lat have been initialized through set_var\")\n",
        "\n",
        "        return da.get_dist_error(self.results, cen_lon=self.cen_lon, cen_lat=self.cen_lat, lon=self.lon, lat=self.lat)\n",
        "\n",
        "    def get_under12_selections(self, age='CENSUS_AGE'):\n",
        "        return da.get_under12_selections(self.results, age=age)\n",
        "\n",
        "class Benchmark():\n",
        "\n",
        "    def __init__(self, match, census, cd):\n",
        "\n",
        "        #Format lat/long for census data\n",
        "        self.census = self.set_census(census)\n",
        "        self.cd = self.set_cd(cd)\n",
        "        self.match = match\n",
        "        self.benchmark = self.create_benchmark()\n",
        "\n",
        "        #Variables to set\n",
        "        self.disambiguated = None\n",
        "        self.confidence = None\n",
        "\n",
        "        #outputs\n",
        "        self.benchmark_results = None\n",
        "        self.confusion_matrix = None\n",
        "\n",
        "    def set_census(self, census):\n",
        "        census = census.loc[:,['CENSUS_MATCH_ADDR', 'CENSUS_Y', 'CENSUS_X']].drop_duplicates()  # select diff variables\n",
        "        census.loc[census.CENSUS_Y > 1000, 'CENSUS_Y'] = 40.799935\n",
        "        return census\n",
        "\n",
        "    def set_cd(self, cd):\n",
        "        return cd[['OBJECTID', 'LONG', 'LAT', \"CD_BLOCK_NUM\"]]\n",
        "\n",
        "    def set_disambiguated(self, disambiguated):\n",
        "        self.disambiguated = disambiguated\n",
        "\n",
        "    def set_confidence(self, confidence):\n",
        "        self.confidence = confidence\n",
        "        # Need to figure out what's going on here\n",
        "        self.benchmark['confidence_score'] = self.benchmark['add_match'] #+ self.benchmark[self.confidence]\n",
        "\n",
        "    def create_benchmark(self):\n",
        "\n",
        "        benchmark = self.match.merge(self.census, how='left', on='CENSUS_MATCH_ADDR', validate='many_to_one')\n",
        "\n",
        "        benchmark['cd_hn'] = benchmark.apply(lambda row: bm.get_hn(row.MATCH_ADDR), axis=1)\n",
        "        benchmark['cen_hn'] = benchmark.apply(lambda row: bm.get_hn(row.CENSUS_MATCH_ADDR), axis=1)\n",
        "        benchmark['cd_add_cln'] = benchmark.apply(lambda row: bm.get_st(row.MATCH_ADDR), axis=1)\n",
        "        benchmark['cen_add_cln'] = benchmark.apply(lambda row: bm.get_st(row.CENSUS_MATCH_ADDR), axis=1)\n",
        "\n",
        "        benchmark['add_match'] = np.where(benchmark.cd_hn == benchmark.cen_hn, 0.1, 0) + np.where(\n",
        "            benchmark.cen_add_cln == benchmark.cd_add_cln, 0.9, 0)\n",
        "\n",
        "        benchmark.merge(self.cd, how='left', on='OBJECTID', validate='many_to_one')\n",
        "\n",
        "        return benchmark\n",
        "\n",
        "    def run_benchmarking(self):\n",
        "        if self.confidence is None:\n",
        "            raise Exception(\"Please set confidence first\")\n",
        "        if self.disambiguated is None:\n",
        "            raise Exception(\"Please set disambiguated first\")\n",
        "\n",
        "        self.benchmark_results = da.get_dist_based_match(self.benchmark, lon = \"LONG\", lat = \"LAT\")['results']\n",
        "        self.confusion_matrix = da.compare_selections(self.disambiguated, self.benchmark_results)['confusion_matrix']\n",
        "\n",
        "    def get_benchmark(self):\n",
        "        return self.benchmark\n",
        "\n",
        "    def get_benchmark_results(self):\n",
        "        if self.benchmark_results is None:\n",
        "            raise Exception(\"Please run benchmarking first\")\n",
        "        return self.benchmark_results\n",
        "\n",
        "    def get_confusion_matrix(self):\n",
        "        if self.confusion_matrix is None:\n",
        "            raise Exception(\"Please run benchmarking first\")\n",
        "        return self.confusion_matrix\n",
        "\n",
        "#version for new run where cd/census information does not need to be joined in\n",
        "class Benchmark_v02():\n",
        "\n",
        "    def __init__(self, match):\n",
        "\n",
        "        self.match = match\n",
        "        self.benchmark = self.create_benchmark()\n",
        "\n",
        "        #Variables to set\n",
        "        self.disambiguated = None\n",
        "        self.confidence = None\n",
        "\n",
        "        #outputs\n",
        "        self.benchmark_results = None\n",
        "        self.confusion_matrix = None\n",
        "\n",
        "    def set_disambiguated(self, disambiguated):\n",
        "        self.disambiguated = disambiguated\n",
        "\n",
        "    def set_confidence(self, confidence):\n",
        "        self.confidence = confidence\n",
        "        # Need to figure out what's going on here\n",
        "        self.benchmark['confidence_score'] = self.benchmark['add_match'] #+ self.benchmark[self.confidence]\n",
        "\n",
        "    def create_benchmark(self):\n",
        "        benchmark = self.match.copy()\n",
        "\n",
        "        #change to use actual street/house numbers once they are sorted out for the census data\n",
        "        benchmark['cd_hn'] = benchmark.apply(lambda row: bm.get_hn(row.CD_H_ADDRESS), axis=1)\n",
        "        benchmark['cen_hn'] = benchmark.apply(lambda row: bm.get_hn(row.CENSUS_MATCH_ADDR), axis=1)\n",
        "        benchmark['cd_add_cln'] = benchmark.apply(lambda row: bm.get_st(row.CD_H_ADDRESS), axis=1)\n",
        "        benchmark['cen_add_cln'] = benchmark.apply(lambda row: bm.get_st(row.CENSUS_MATCH_ADDR), axis=1)\n",
        "\n",
        "        benchmark['add_match'] = np.where(benchmark.cd_hn == benchmark.cen_hn, 0.1, 0) + np.where(\n",
        "            benchmark.cen_add_cln == benchmark.cd_add_cln, 0.9, 0)\n",
        "\n",
        "        return benchmark\n",
        "\n",
        "    def run_benchmarking(self):\n",
        "        if self.confidence is None:\n",
        "            raise Exception(\"Please set confidence first\")\n",
        "        if self.disambiguated is None:\n",
        "            raise Exception(\"Please set disambiguated first\")\n",
        "\n",
        "        self.benchmark_results = da.get_dist_based_match(self.benchmark, confidence = self.confidence)['results']\n",
        "        self.confusion_matrix = da.compare_selections(self.disambiguated, self.benchmark_results)['confusion_matrix']\n",
        "\n",
        "    def get_benchmark(self):\n",
        "        return self.benchmark\n",
        "\n",
        "    def get_benchmark_results(self):\n",
        "        if self.benchmark_results is None:\n",
        "            raise Exception(\"Please run benchmarking first\")\n",
        "        return self.benchmark_results\n",
        "\n",
        "    def get_confusion_matrix(self):\n",
        "        if self.confusion_matrix is None:\n",
        "            raise Exception(\"Please run benchmarking first\")\n",
        "        return self.confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing __init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__HwRP4lYRD"
      },
      "source": [
        "### confidence score tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHBourqWlMt1",
        "outputId": "344f4fc0-0cc5-46a6-ea50-2f6c58c57587"
      },
      "source": [
        "%%writefile confidence_score_tuning.py\n",
        "from __init__ import Disambiguator, Disambiguator1880, Benchmark, Benchmark_v02\n",
        "\n",
        "\"\"\"\n",
        "Purpose: Generate confidence scores as a list\n",
        "df: elastic search dataframe formatted for disambiguation\n",
        "columns: columns we want to use to create confidence score\n",
        "weights: corresponding weights we want to use to create confidence score, should sum to one\n",
        "\"\"\"\n",
        "def confidence_score(df, columns, weights):\n",
        "    return [sum(row[col]*w for col, w in zip(columns,weights)) for index,row in df.iterrows()]\n",
        "\n",
        "\"\"\"\n",
        "#Unneeded in new data run\n",
        "Purpose: Format census data for benchmarking\n",
        "census: census data \n",
        "\"\"\"\n",
        "def census_for_disamb(census):\n",
        "    census_latlng_tuning = census.copy()\n",
        "    census_latlng_tuning['CENSUS_ID'] = 'CENSUS_' + census_latlng_tuning['OBJECTID.x'].astype(str)\n",
        "    census_latlng_tuning = census_latlng_tuning.loc[:, ['CENSUS_ID', 'CENSUS_X', 'CENSUS_Y']]\n",
        "    census_latlng_tuning.loc[census_latlng_tuning.CENSUS_Y > 1000, 'CENSUS_Y'] = 40.799935\n",
        "    return census_latlng_tuning\n",
        "\n",
        "\"\"\"\n",
        "param_grid: list of dictionaries with names of columns to use for a trial cf score and corresponding weights\n",
        "df_allcols: elastic search output formatted for disambiguation\n",
        "df_census: census data for benchmarking\n",
        "df_cd: city directory data for benchmarking\n",
        "\"\"\"\n",
        "def confidence_score_tuning(param_grid, df_allcols, df_census, df_cd):\n",
        "    # Store results\n",
        "    results = {}\n",
        "    df = df_allcols.copy()\n",
        "\n",
        "    # Get confidence score for each value in grid\n",
        "    for i in range(len(param_grid)):\n",
        "        name = \"confidence_score_\" + str(i)\n",
        "        df.loc[:, name] = confidence_score(df_allcols, param_grid[i][\"columns\"], param_grid[i][\"weights\"])\n",
        "\n",
        "    # Create benchmark object\n",
        "    benchmark = Benchmark(df, df_census, df_cd)\n",
        "\n",
        "    # Format census data for tuning\n",
        "    census_tuning = census_for_disamb(df_census)\n",
        "\n",
        "    # try:\n",
        "    for i in range(len(param_grid)):\n",
        "        name = \"confidence_score_\" + str(i)\n",
        "\n",
        "        # Run disambiguation process (use betweeness and clustering -- based on Jolene's work)\n",
        "        basic = Disambiguator1880(df, confidence=name)\n",
        "\n",
        "        try:\n",
        "            basic.run_disambiguation()\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        result = basic.get_result()  # .to_csv(\"..data/confidence_score_tuning/confidence_score_\"+str(i))\n",
        "\n",
        "        # Results analysis\n",
        "        basic.merge_census_var(census_tuning)\n",
        "        basic.set_var()\n",
        "\n",
        "        # benchmarking\n",
        "        benchmark.set_confidence(name)\n",
        "        benchmark.set_disambiguated(result)\n",
        "        benchmark.run_benchmarking()\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\"columns\": param_grid[i][\"columns\"], \"weights\": param_grid[i][\"weights\"],\n",
        "                         \"Match Rate\": basic.get_match_rate(), \"Address Success\": basic.get_addr_success(),\n",
        "                         \"Under 12\": basic.get_under12_selections(),\n",
        "                         \"confusion matrix\": benchmark.get_confusion_matrix()}\n",
        "\n",
        "        # will return results so far even if exception occurs\n",
        "        # Spit out the best columns and weights (Add this in when decide what makes something the best)\n",
        "        # For now simply output the analysis\n",
        "    return results\n",
        "\n",
        "#Uses new version of benchmarking, bc elastic search output means we don't need to join in the x/ys separately\n",
        "def confidence_score_tuning_v02(param_grid, df_elastic_search):\n",
        "    # Store results\n",
        "    results = {}\n",
        "    df = df_elastic_search.copy()\n",
        "\n",
        "    # Get confidence score for each value in grid\n",
        "    for i in range(len(param_grid)):\n",
        "        name = \"confidence_score_\" + str(i)\n",
        "        df.loc[:, name] = confidence_score(df_elastic_search, param_grid[i][\"columns\"], param_grid[i][\"weights\"])\n",
        "\n",
        "    benchmark = Benchmark_v02(df_elastic_search)\n",
        "\n",
        "    for i in range(len(param_grid)):\n",
        "        name = \"confidence_score_\" + str(i)\n",
        "\n",
        "        # Run disambiguation process (use betweeness and clustering -- based on Jolene's work)\n",
        "        basic = Disambiguator1880(df, confidence=name)\n",
        "\n",
        "        #try:\n",
        "        basic.run_disambiguation()\n",
        "        #except:\n",
        "            #continue\n",
        "\n",
        "        result = basic.get_result()  # .to_csv(\"..data/confidence_score_tuning/confidence_score_\"+str(i))\n",
        "\n",
        "        # Results analysis\n",
        "        basic.set_var()\n",
        "\n",
        "        # benchmarking\n",
        "        benchmark.set_confidence(name)\n",
        "        benchmark.set_disambiguated(result)\n",
        "        benchmark.run_benchmarking()\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\"columns\": param_grid[i][\"columns\"], \"weights\": param_grid[i][\"weights\"],\n",
        "                         \"Match Rate\": basic.get_match_rate(), \"Address Success\": basic.get_addr_success(),\n",
        "                         \"Under 12\": basic.get_under12_selections(),\n",
        "                         \"confusion matrix\": benchmark.get_confusion_matrix()}\n",
        "\n",
        "        # will return results so far even if exception occurs\n",
        "        # Spit out the best columns and weights (Add this in when decide what makes something the best)\n",
        "        # For now simply output the analysis\n",
        "    return results\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing confidence_score_tuning.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x6cDOym6YZd"
      },
      "source": [
        "## confidence scoring module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdCPjXGjPeBN"
      },
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import disambiguation\n",
        "from __init__ import Disambiguator, Disambiguator1880\n",
        "import analysis as da\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import processing as dp \n",
        "from __init__ import Benchmark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhHQnJBpSlT-"
      },
      "source": [
        "### Get and Format Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kawe07E2Po6d"
      },
      "source": [
        "#elastic_match = pd.read_csv(\"../../Data/matches.csv\")\n",
        "# used old 1880 files. still need to run confidence score tuning on new 1880 matched file\n",
        "\n",
        "elastic_match = pd.read_csv(\"/content/es-1880-21-5-2020.csv\", sep='\\t', engine='python')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBqsE71CVSCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea81b3b-a410-492b-d794-15351b296109"
      },
      "source": [
        "census = pd.read_csv(\"/content/census_1880_mn_v04.csv\")\n",
        "def census_for_disamb(census):\n",
        "    census_latlng_tuning = census.copy()\n",
        "    census_latlng_tuning['CENSUS_ID'] = 'CENSUS_' + census_latlng_tuning['OBJECTID.x'].astype(str)\n",
        "    census_latlng_tuning = census_latlng_tuning.loc[:, ['CENSUS_ID', 'CENSUS_X', 'CENSUS_Y']]\n",
        "    census_latlng_tuning.loc[census_latlng_tuning.CENSUS_Y > 1000, 'CENSUS_Y'] = 40.799935\n",
        "    return census_latlng_tuning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA-3rtX5YW_S"
      },
      "source": [
        "#cd_latlng\n",
        "latlng = pd.read_csv(\"/content/cd_1880_mn_v04.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ymerUpZK7h"
      },
      "source": [
        "match = dp.elastic_to_disamb(elastic_match, latlng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH5NytekY_kt"
      },
      "source": [
        "##### create a sample of wards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdoXguy-ozYA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "338589db-ad48-46f4-bfe7-90804c0722ea"
      },
      "source": [
        "match.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OBJECTID.x</th>\n",
              "      <th>CENSUS_NAMEFRSTB</th>\n",
              "      <th>CENSUS_NAMELASTB</th>\n",
              "      <th>CENSUS_AGE</th>\n",
              "      <th>CENSUS_OCCLABELB</th>\n",
              "      <th>CENSUS_MATCH_ADDR</th>\n",
              "      <th>CENSUS_SEGMENT_ID</th>\n",
              "      <th>WARD_NUM</th>\n",
              "      <th>CD_ED</th>\n",
              "      <th>OBJECTID</th>\n",
              "      <th>MATCH_ADDR</th>\n",
              "      <th>CD_FIRST_NAME</th>\n",
              "      <th>CD_LAST_NAME</th>\n",
              "      <th>CD_OCCUPATION</th>\n",
              "      <th>CD_FINAL_HOUSENUM</th>\n",
              "      <th>jw_fn</th>\n",
              "      <th>jw_ln</th>\n",
              "      <th>jw_score</th>\n",
              "      <th>occ_listed</th>\n",
              "      <th>age_score</th>\n",
              "      <th>cd_count</th>\n",
              "      <th>census_count</th>\n",
              "      <th>confidence_score</th>\n",
              "      <th>CD_ID</th>\n",
              "      <th>CENSUS_ID</th>\n",
              "      <th>census_count_inverse</th>\n",
              "      <th>cd_count_inverse</th>\n",
              "      <th>LONG</th>\n",
              "      <th>LAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>862548</td>\n",
              "      <td>STEPHEN</td>\n",
              "      <td>ZELLER</td>\n",
              "      <td>40</td>\n",
              "      <td>IRON MOULDER</td>\n",
              "      <td>504 55TH ST W, NYC-Manhattan, NY</td>\n",
              "      <td>3789</td>\n",
              "      <td>22</td>\n",
              "      <td>513.0</td>\n",
              "      <td>3</td>\n",
              "      <td>504 W 55 ST, New York, NY</td>\n",
              "      <td>Stephen</td>\n",
              "      <td>Zoller</td>\n",
              "      <td>molder</td>\n",
              "      <td>504</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.904</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.95</td>\n",
              "      <td>CD_3</td>\n",
              "      <td>CENSUS_862548</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-73.989856</td>\n",
              "      <td>40.767868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>795510</td>\n",
              "      <td>OSCAR</td>\n",
              "      <td>ZOLLIKOFFER</td>\n",
              "      <td>70</td>\n",
              "      <td>PRESIDENT, METROPOLITAN GAS CO.</td>\n",
              "      <td>210 46TH ST W, NYC-Manhattan, NY</td>\n",
              "      <td>3357</td>\n",
              "      <td>22</td>\n",
              "      <td>469.0</td>\n",
              "      <td>4</td>\n",
              "      <td>210 W 46 ST, New York, NY</td>\n",
              "      <td>Oscar</td>\n",
              "      <td>Zollikoffer</td>\n",
              "      <td>pres</td>\n",
              "      <td>210</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>CD_4</td>\n",
              "      <td>CENSUS_795510</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-73.985856</td>\n",
              "      <td>40.758819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>978306</td>\n",
              "      <td>OSCAR</td>\n",
              "      <td>ZOLLIKOFFER</td>\n",
              "      <td>33</td>\n",
              "      <td>SECRETARY, MET. GAS LIGHT CO.</td>\n",
              "      <td>65 54TH ST W, NYC-Manhattan, NY</td>\n",
              "      <td>3488</td>\n",
              "      <td>19</td>\n",
              "      <td>582.0</td>\n",
              "      <td>6</td>\n",
              "      <td>65 W 54 ST, New York, NY</td>\n",
              "      <td>Oscar</td>\n",
              "      <td>Zollikoffer</td>\n",
              "      <td>sec</td>\n",
              "      <td>65</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>CD_6</td>\n",
              "      <td>CENSUS_978306</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-73.977969</td>\n",
              "      <td>40.762224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56054</td>\n",
              "      <td>ROBERT</td>\n",
              "      <td>ZALLER</td>\n",
              "      <td>31</td>\n",
              "      <td>BIRD IMPORTER</td>\n",
              "      <td>5 WILLIAM ST N, NYC-Manhattan, NY</td>\n",
              "      <td>531</td>\n",
              "      <td>4</td>\n",
              "      <td>36.0</td>\n",
              "      <td>8</td>\n",
              "      <td>5 N William St, New York, NY</td>\n",
              "      <td>Robert</td>\n",
              "      <td>Zoller</td>\n",
              "      <td>birds</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.940</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.97</td>\n",
              "      <td>CD_8</td>\n",
              "      <td>CENSUS_56054</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-74.004401</td>\n",
              "      <td>40.711743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>522583</td>\n",
              "      <td>LEOPOLD</td>\n",
              "      <td>ZOLLMANN</td>\n",
              "      <td>30</td>\n",
              "      <td>SHOE MAKER</td>\n",
              "      <td>342 HOUSTON ST E, NYC-Manhattan, NY</td>\n",
              "      <td>1283</td>\n",
              "      <td>11</td>\n",
              "      <td>305.0</td>\n",
              "      <td>13</td>\n",
              "      <td>344 E HOUSTON ST, New York, NY</td>\n",
              "      <td>Leopold</td>\n",
              "      <td>Zollmann</td>\n",
              "      <td>shoes</td>\n",
              "      <td>344</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.90</td>\n",
              "      <td>CD_13</td>\n",
              "      <td>CENSUS_522583</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-73.981913</td>\n",
              "      <td>40.720894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   OBJECTID.x CENSUS_NAMEFRSTB  ...       LONG        LAT\n",
              "0      862548          STEPHEN  ... -73.989856  40.767868\n",
              "1      795510            OSCAR  ... -73.985856  40.758819\n",
              "2      978306            OSCAR  ... -73.977969  40.762224\n",
              "3       56054           ROBERT  ... -74.004401  40.711743\n",
              "4      522583          LEOPOLD  ... -73.981913  40.720894\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z89EQHxY68K"
      },
      "source": [
        "wards = [3,9,10,18,21,22]\n",
        "match_sample = match[match.WARD_NUM.isin(wards)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFgbkHejiPP-"
      },
      "source": [
        "### confidence score tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EsrVy8UiJEw"
      },
      "source": [
        "#function to get confidence score including specified columns and weights\n",
        "def confidence_score(df, columns, weights):\n",
        "    return [sum(row[col]*w for col, w in zip(columns,weights)) for index,row in df.iterrows()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nYUB2sCneXK"
      },
      "source": [
        "def confidence_score_tuning(param_grid, df_allcols, df_census, df_cd):\n",
        "    #Store results\n",
        "    results = {}\n",
        "    df = df_allcols.copy()\n",
        "    \n",
        "    #Get confidence score for each value in grid\n",
        "    for i in range(len(param_grid)):\n",
        "        name = \"confidence_score_\"+str(i)\n",
        "        df.loc[:,name] = confidence_score(df_allcols, param_grid[i][\"columns\"], param_grid[i][\"weights\"])\n",
        "\n",
        "        #print(df.head())\n",
        "        \n",
        "    #Create benchmark object\n",
        "    benchmark = Benchmark(df, df_census, df_cd)\n",
        "    \n",
        "    #Format census data for tuning\n",
        "    census_tuning = census_for_disamb(df_census)\n",
        "    \n",
        "   # try:\n",
        "    for i in range(len(param_grid)):\n",
        "\n",
        "        name = \"confidence_score_\"+str(i)\n",
        "\n",
        "        #Run disambiguation process (use betweeness and clustering -- based on Jolene's work)\n",
        "        basic = Disambiguator1880(df, confidence = name)\n",
        "\n",
        "        try:\n",
        "            basic.run_disambiguation()\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        result = basic.get_result() #.to_csv(\"..data/confidence_score_tuning/confidence_score_\"+str(i))\n",
        "\n",
        "        #Results analysis\n",
        "        basic.merge_census_var(census_tuning)\n",
        "        basic.set_var() \n",
        "\n",
        "        #benchmarking\n",
        "        benchmark.set_confidence(name)\n",
        "        benchmark.set_disambiguated(result)\n",
        "        benchmark.run_benchmarking()\n",
        "\n",
        "        #Store results\n",
        "        results[name] = {\"columns\":param_grid[i][\"columns\"], \"weights\":param_grid[i][\"weights\"], \"Match Rate\":basic.get_match_rate(), \"Address Success\":basic.get_addr_success(),\"Under 12\":basic.get_under12_selections(), \"confusion matrix\":benchmark.get_confusion_matrix()}\n",
        "        \n",
        "    #will return results so far even if exception occurs\n",
        "        #Spit out the best columns and weights (Add this in when decide what makes something the best)\n",
        "        #For now simply output the analysis\n",
        "    return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq6f8Ne2wM5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf6c03f-5242-49ed-caff-168890f19536"
      },
      "source": [
        "#Columns and weights\n",
        "param_grid = [{\"columns\": ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], \"weights\":[0.5,0.2,0.2,0.05,0.05]},\n",
        "              {\"columns\": ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], \"weights\":[0.55,0.18,0.18,0.05,0.04]},\n",
        "             {\"columns\": ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], \"weights\":[0.7,0.1,0.1,0.05,0.05]}, #Best outcome\n",
        "             {\"columns\": ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], \"weights\":[0.6,0.1,0.1,0.1,0.1]},\n",
        "              {\"columns\": ['jw_score', 'occ_listed', 'age_score'], \"weights\":[0.8,0.10,0.10]},\n",
        "             {\"columns\": ['jw_score','census_count_inverse', 'occ_listed', 'age_score'], \"weights\":[0.6,0.15,0.1,0.15]},\n",
        "             {\"columns\": ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], \"weights\":[0.8,0.05,0.05,0.05,0.05]},\n",
        "             {\"columns\": ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], \"weights\":[0.6,0.15,0.15,0.05,0.05]}]\n",
        "\n",
        "             \n",
        "tuning_results = confidence_score_tuning(param_grid, match_sample, census, latlng)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   OBJECTID.x CENSUS_NAMEFRSTB  ...        LAT  confidence_score_0\n",
            "0      862548          STEPHEN  ...  40.767868               0.952\n",
            "1      795510            OSCAR  ...  40.758819               1.000\n",
            "6      330206           HERMAN  ...  40.716711               1.000\n",
            "7      855850          MICHAEL  ...  40.766168               1.000\n",
            "8      796365            RICKA  ...  40.758155               0.967\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_0  confidence_score_1\n",
            "0      862548          STEPHEN  ...              0.952              0.9472\n",
            "1      795510            OSCAR  ...              1.000              1.0000\n",
            "6      330206           HERMAN  ...              1.000              1.0000\n",
            "7      855850          MICHAEL  ...              1.000              1.0000\n",
            "8      796365            RICKA  ...              0.967              0.9637\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_1  confidence_score_2\n",
            "0      862548          STEPHEN  ...             0.9472              0.9328\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9637              0.9538\n",
            "\n",
            "[5 rows x 32 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_2  confidence_score_3\n",
            "0      862548          STEPHEN  ...             0.9328              0.9424\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9538              0.9604\n",
            "\n",
            "[5 rows x 33 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_3  confidence_score_4\n",
            "0      862548          STEPHEN  ...             0.9424              0.9232\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9604              0.9472\n",
            "\n",
            "[5 rows x 34 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_4  confidence_score_5\n",
            "0      862548          STEPHEN  ...             0.9232              0.9424\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9472              0.9604\n",
            "\n",
            "[5 rows x 35 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_5  confidence_score_6\n",
            "0      862548          STEPHEN  ...             0.9424              0.9232\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9604              0.9472\n",
            "\n",
            "[5 rows x 36 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_6  confidence_score_7\n",
            "0      862548          STEPHEN  ...             0.9232              0.9424\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9472              0.9604\n",
            "\n",
            "[5 rows x 37 columns]\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 132.70141673088074\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 134.90750765800476\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 131.44855093955994\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 0\n",
            "Cleaning output (3/4)...\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 26289\n",
            "Reached: 0\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 23671\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Reached: 16000\n",
            "Reached: 17000\n",
            "Reached: 18000\n",
            "Reached: 19000\n",
            "Reached: 20000\n",
            "Reached: 21000\n",
            "Reached: 22000\n",
            "Reached: 23000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 152.27067732810974\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 0\n",
            "Cleaning output (3/4)...\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 132.86499214172363\n",
            "Done! :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtlvJ6HhwM4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "c67a99a8-25e2-49d9-de6f-1cf40f40c578"
      },
      "source": [
        "display(pd.DataFrame.from_dict(tuning_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>confidence_score_0</th>\n",
              "      <th>confidence_score_1</th>\n",
              "      <th>confidence_score_2</th>\n",
              "      <th>confidence_score_5</th>\n",
              "      <th>confidence_score_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>columns</th>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "      <td>[jw_score, census_count_inverse, occ_listed, a...</td>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weights</th>\n",
              "      <td>[0.5, 0.2, 0.2, 0.05, 0.05]</td>\n",
              "      <td>[0.55, 0.18, 0.18, 0.05, 0.04]</td>\n",
              "      <td>[0.7, 0.1, 0.1, 0.05, 0.05]</td>\n",
              "      <td>[0.6, 0.15, 0.1, 0.15]</td>\n",
              "      <td>[0.6, 0.15, 0.15, 0.05, 0.05]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Match Rate</th>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Address Success</th>\n",
              "      <td>{'n_perfect_match_chosen': 8445, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8443, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8445, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8441, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8446, 'n_perfect_ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Under 12</th>\n",
              "      <td>2.6</td>\n",
              "      <td>2.68</td>\n",
              "      <td>2.67</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion matrix</th>\n",
              "      <td>[[30642, 3776], [3776, 11333]]</td>\n",
              "      <td>[[30627, 3791], [3791, 11318]]</td>\n",
              "      <td>[[30633, 3785], [3785, 11324]]</td>\n",
              "      <td>[[30591, 3827], [3827, 11282]]</td>\n",
              "      <td>[[30634, 3784], [3784, 11325]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 confidence_score_0  ...                                 confidence_score_7\n",
              "columns           [jw_score, cd_count_inverse, census_count_inve...  ...  [jw_score, cd_count_inverse, census_count_inve...\n",
              "weights                                 [0.5, 0.2, 0.2, 0.05, 0.05]  ...                      [0.6, 0.15, 0.15, 0.05, 0.05]\n",
              "Match Rate                                                    98.48  ...                                              98.48\n",
              "Address Success   {'n_perfect_match_chosen': 8445, 'n_perfect_ma...  ...  {'n_perfect_match_chosen': 8446, 'n_perfect_ma...\n",
              "Under 12                                                        2.6  ...                                               2.65\n",
              "confusion matrix                     [[30642, 3776], [3776, 11333]]  ...                     [[30634, 3784], [3784, 11325]]\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EkttjrRwM3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318a982a-9cd2-4412-bfd5-cdbb2cf910ac"
      },
      "source": [
        "benchmark_test = confidence_score_tuning(param_grid, match_sample, census, latlng)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   OBJECTID.x CENSUS_NAMEFRSTB  ...        LAT  confidence_score_0\n",
            "0      862548          STEPHEN  ...  40.767868               0.952\n",
            "1      795510            OSCAR  ...  40.758819               1.000\n",
            "6      330206           HERMAN  ...  40.716711               1.000\n",
            "7      855850          MICHAEL  ...  40.766168               1.000\n",
            "8      796365            RICKA  ...  40.758155               0.967\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_0  confidence_score_1\n",
            "0      862548          STEPHEN  ...              0.952              0.9472\n",
            "1      795510            OSCAR  ...              1.000              1.0000\n",
            "6      330206           HERMAN  ...              1.000              1.0000\n",
            "7      855850          MICHAEL  ...              1.000              1.0000\n",
            "8      796365            RICKA  ...              0.967              0.9637\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_1  confidence_score_2\n",
            "0      862548          STEPHEN  ...             0.9472              0.9328\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9637              0.9538\n",
            "\n",
            "[5 rows x 32 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_2  confidence_score_3\n",
            "0      862548          STEPHEN  ...             0.9328              0.9424\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9538              0.9604\n",
            "\n",
            "[5 rows x 33 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_3  confidence_score_4\n",
            "0      862548          STEPHEN  ...             0.9424              0.9232\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9604              0.9472\n",
            "\n",
            "[5 rows x 34 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_4  confidence_score_5\n",
            "0      862548          STEPHEN  ...             0.9232              0.9424\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9472              0.9604\n",
            "\n",
            "[5 rows x 35 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_5  confidence_score_6\n",
            "0      862548          STEPHEN  ...             0.9424              0.9232\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9604              0.9472\n",
            "\n",
            "[5 rows x 36 columns]\n",
            "   OBJECTID.x CENSUS_NAMEFRSTB  ... confidence_score_6  confidence_score_7\n",
            "0      862548          STEPHEN  ...             0.9232              0.9424\n",
            "1      795510            OSCAR  ...             1.0000              1.0000\n",
            "6      330206           HERMAN  ...             1.0000              1.0000\n",
            "7      855850          MICHAEL  ...             1.0000              1.0000\n",
            "8      796365            RICKA  ...             0.9472              0.9604\n",
            "\n",
            "[5 rows x 37 columns]\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 132.74643087387085\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 132.62286901474\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 135.6656904220581\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 0\n",
            "Cleaning output (3/4)...\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 26289\n",
            "Reached: 0\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 23671\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Reached: 16000\n",
            "Reached: 17000\n",
            "Reached: 18000\n",
            "Reached: 19000\n",
            "Reached: 20000\n",
            "Reached: 21000\n",
            "Reached: 22000\n",
            "Reached: 23000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 152.85495948791504\n",
            "Done! :)\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 0\n",
            "Cleaning output (3/4)...\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 15333\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Reached: 7000\n",
            "Reached: 8000\n",
            "Reached: 9000\n",
            "Reached: 10000\n",
            "Reached: 11000\n",
            "Reached: 12000\n",
            "Reached: 13000\n",
            "Reached: 14000\n",
            "Reached: 15000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 133.69159078598022\n",
            "Done! :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhcK7u7IwGtA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "08480378-37da-467a-d8da-966346f778ae"
      },
      "source": [
        "display(pd.DataFrame.from_dict(benchmark_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>confidence_score_0</th>\n",
              "      <th>confidence_score_1</th>\n",
              "      <th>confidence_score_2</th>\n",
              "      <th>confidence_score_5</th>\n",
              "      <th>confidence_score_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>columns</th>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "      <td>[jw_score, census_count_inverse, occ_listed, a...</td>\n",
              "      <td>[jw_score, cd_count_inverse, census_count_inve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weights</th>\n",
              "      <td>[0.5, 0.2, 0.2, 0.05, 0.05]</td>\n",
              "      <td>[0.55, 0.18, 0.18, 0.05, 0.04]</td>\n",
              "      <td>[0.7, 0.1, 0.1, 0.05, 0.05]</td>\n",
              "      <td>[0.6, 0.15, 0.1, 0.15]</td>\n",
              "      <td>[0.6, 0.15, 0.15, 0.05, 0.05]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Match Rate</th>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "      <td>98.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Address Success</th>\n",
              "      <td>{'n_perfect_match_chosen': 8445, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8443, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8445, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8441, 'n_perfect_ma...</td>\n",
              "      <td>{'n_perfect_match_chosen': 8446, 'n_perfect_ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Under 12</th>\n",
              "      <td>2.6</td>\n",
              "      <td>2.68</td>\n",
              "      <td>2.67</td>\n",
              "      <td>2.45</td>\n",
              "      <td>2.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion matrix</th>\n",
              "      <td>[[30642, 3776], [3776, 11333]]</td>\n",
              "      <td>[[30627, 3791], [3791, 11318]]</td>\n",
              "      <td>[[30633, 3785], [3785, 11324]]</td>\n",
              "      <td>[[30591, 3827], [3827, 11282]]</td>\n",
              "      <td>[[30634, 3784], [3784, 11325]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 confidence_score_0  ...                                 confidence_score_7\n",
              "columns           [jw_score, cd_count_inverse, census_count_inve...  ...  [jw_score, cd_count_inverse, census_count_inve...\n",
              "weights                                 [0.5, 0.2, 0.2, 0.05, 0.05]  ...                      [0.6, 0.15, 0.15, 0.05, 0.05]\n",
              "Match Rate                                                    98.48  ...                                              98.48\n",
              "Address Success   {'n_perfect_match_chosen': 8445, 'n_perfect_ma...  ...  {'n_perfect_match_chosen': 8446, 'n_perfect_ma...\n",
              "Under 12                                                        2.6  ...                                               2.65\n",
              "confusion matrix                     [[30642, 3776], [3776, 11333]]  ...                     [[30634, 3784], [3784, 11325]]\n",
              "\n",
              "[6 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wogwphiNrtxn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCeWKAKpwaca"
      },
      "source": [
        "## 1850 disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxs32wfErvLg",
        "outputId": "2da1d944-0ef1-443f-cac9-5fd9fce904f6"
      },
      "source": [
        "# disambiguation on 22-09 summer file run\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "import pandas as pd\n",
        "from __init__ import Disambiguator\n",
        "import confidence_score_tuning as cf\n",
        "import processing as dp\n",
        "\n",
        "\n",
        "elastic_search_file = \"/content/es-1850-22-9-2020.csv\"\n",
        "match_file = \"/content/1850_mn_match_v2.csv\"\n",
        "\n",
        "elastic_match = pd.read_csv(elastic_search_file, sep='\\t', engine='python')\n",
        "\n",
        "elastic_match = dp.col_for_disamb(elastic_match, cd_id = \"CD_RECORD_ID\",cen_id = \"CENSUS_IPUMS_UID\")\n",
        "elastic_match.loc[:,\"confidence_score\"] = cf.confidence_score(elastic_match, ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], [0.7,0.1,0.1,0.05,0.05])\n",
        "\n",
        "print (\"No. of matches: \" + str(len(elastic_match)))\n",
        "print (\"No. of unique CD records: \" + str(len(elastic_match['OBJECTID'].drop_duplicates())))\n",
        "print (\"No. of unique Census records: \" + str(len(elastic_match['CENSUS_IPUMS_UID'].drop_duplicates())))\n",
        "print (\"No. of 1:1 matches: \" + str(len(elastic_match[ (elastic_match['census_count'] == 1) & (elastic_match['cd_count'] == 1) ] )) )\n",
        "print (\"No. of matches where census record <= 12: \" + str( len(elastic_match[elastic_match['CENSUS_AGE'] <= 12]) ))\n",
        "print (\"No. of unique matches where census record <= 12: \" + str( len(elastic_match[elastic_match['CENSUS_AGE'] <= 12]['CENSUS_IPUMS_UID'].drop_duplicates()) ))\n",
        "print (\"No. of anchors (confidence score = 1): \" + str( len(elastic_match[elastic_match['confidence_score'] == 1]) ))\n",
        "\n",
        "disambiguate = Disambiguator(elastic_match, lon='CD_X', lat='CD_Y',sort_var='CENSUS_INDEX')\n",
        "disambiguate.run_disambiguation()\n",
        "\n",
        "result = disambiguate.get_result()\n",
        "\n",
        "print(\"records with a final match:\", sum(result.selected))\n",
        "print(\"all records matched:\", len(result))\n",
        "print(\"number of cd records matched:\", len(result['CD_ID'].drop_duplicates()))\n",
        "print(\"proportion of cd records in elastic search included in final match:\", sum(result.selected) / len(result['CD_ID'].drop_duplicates()))\n",
        "\n",
        "result.to_csv(match_file, index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of matches: 63312\n",
            "No. of unique CD records: 25089\n",
            "No. of unique Census records: 47306\n",
            "No. of 1:1 matches: 11973\n",
            "No. of matches where census record <= 12: 14877\n",
            "No. of unique matches where census record <= 12: 10627\n",
            "No. of anchors (confidence score = 1): 6702\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 6702\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 297.37249851226807\n",
            "Done! :)\n",
            "records with a final match: 24585.0\n",
            "all records matched: 63312\n",
            "number of cd records matched: 25089\n",
            "proportion of cd records in elastic search included in final match: 0.9799115150065766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qz6TMAqOIto",
        "outputId": "69294233-1486-48cd-87a9-ab4e89f14f63"
      },
      "source": [
        "# disambiguation on 24-11 fall file run\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "import pandas as pd\n",
        "from __init__ import Disambiguator\n",
        "import confidence_score_tuning as cf\n",
        "import processing as dp\n",
        "\n",
        "\n",
        "elastic_search_file = \"/content/es-1850-24-11-2020.csv\"\n",
        "match_file = \"/content/1850_mn_match_24-11-2020.csv\"\n",
        "\n",
        "elastic_match = pd.read_csv(elastic_search_file)\n",
        "\n",
        "\n",
        "elastic_match = dp.col_for_disamb(elastic_match, cd_id = \"CD_RECORD_ID\",cen_id = \"CENSUS_IPUMS_UID\")\n",
        "elastic_match.loc[:,\"confidence_score\"] = cf.confidence_score(elastic_match, ['jw_score','cd_count_inverse','census_count_inverse', 'occ_listed', 'age_score'], [0.7,0.1,0.1,0.05,0.05])\n",
        "\n",
        "print (\"No. of matches: \" + str(len(elastic_match)))\n",
        "print (\"No. of unique CD records: \" + str(len(elastic_match['OBJECTID'].drop_duplicates())))\n",
        "print (\"No. of unique Census records: \" + str(len(elastic_match['CENSUS_IPUMS_UID'].drop_duplicates())))\n",
        "print (\"No. of 1:1 matches: \" + str(len(elastic_match[ (elastic_match['census_count'] == 1) & (elastic_match['cd_count'] == 1) ] )) )\n",
        "print (\"No. of matches where census record <= 12: \" + str( len(elastic_match[elastic_match['CENSUS_AGE'] <= 12]) ))\n",
        "print (\"No. of unique matches where census record <= 12: \" + str( len(elastic_match[elastic_match['CENSUS_AGE'] <= 12]['CENSUS_IPUMS_UID'].drop_duplicates()) ))\n",
        "print (\"No. of anchors (confidence score = 1): \" + str( len(elastic_match[elastic_match['confidence_score'] == 1]) ))\n",
        "\n",
        "disambiguate = Disambiguator(elastic_match, lon='CD_X', lat='CD_Y',sort_var='CENSUS_INDEX')\n",
        "disambiguate.run_disambiguation()\n",
        "\n",
        "result = disambiguate.get_result()\n",
        "\n",
        "print(\"records with a final match:\", sum(result.selected))\n",
        "print(\"all records matched:\", len(result))\n",
        "print(\"number of cd records matched:\", len(result['CD_ID'].drop_duplicates()))\n",
        "print(\"proportion of cd records in elastic search included in final match:\", sum(result.selected) / len(result['CD_ID'].drop_duplicates()))\n",
        "\n",
        "result.to_csv(match_file, index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of matches: 89294\n",
            "No. of unique CD records: 32706\n",
            "No. of unique Census records: 65162\n",
            "No. of 1:1 matches: 14737\n",
            "No. of matches where census record <= 12: 21681\n",
            "No. of unique matches where census record <= 12: 15281\n",
            "No. of anchors (confidence score = 1): 6725\n",
            "Running\n",
            "Creating dictionary of sub dfs (1/4)...\n",
            "Applying algorithms iteratively (2/4)...\n",
            "Number of Subgraphs: 6725\n",
            "Reached: 0\n",
            "Reached: 1000\n",
            "Reached: 2000\n",
            "Reached: 3000\n",
            "Reached: 4000\n",
            "Reached: 5000\n",
            "Reached: 6000\n",
            "Cleaning output (3/4)...\n",
            "Disambiguating (4/4)...\n",
            "Total time: 458.5079038143158\n",
            "Done! :)\n",
            "records with a final match: 31818.0\n",
            "all records matched: 89294\n",
            "number of cd records matched: 32706\n",
            "proportion of cd records in elastic search included in final match: 0.9728490185287103\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}